{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4a4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from src.utils.data_loader import InsuranceDataProcessor\n",
    "from src.analysis.eda_analyzer import EDAAnalyzer\n",
    "from src.visualization.eda_plots import EDAVisualizer\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134c8ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 13:51:03,167 - src.utils.data_loader - INFO - Loading data from data\\raw\\insurance_data.csv\n",
      "2025-12-06 13:51:03,171 - src.utils.data_loader - ERROR - Error loading data: [Errno 2] No such file or directory: 'data\\\\raw\\\\insurance_data.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 1: DATA LOADING AND VALIDATION\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\raw\\\\insurance_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m processor \u001b[38;5;241m=\u001b[39m InsuranceDataProcessor(config_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../config/config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Display basic information\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\insurance-risk-analysis-week3\\notebooks\\..\\src\\utils\\data_loader.py:50\u001b[0m, in \u001b[0;36mInsuranceDataProcessor.load_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Try different file formats\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 50\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path)\n",
      "File \u001b[1;32mc:\\Users\\admin\\insurance-risk-analysis-week3\\my_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\insurance-risk-analysis-week3\\my_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\admin\\insurance-risk-analysis-week3\\my_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\insurance-risk-analysis-week3\\my_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\admin\\insurance-risk-analysis-week3\\my_env\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\raw\\\\insurance_data.csv'"
     ]
    }
   ],
   "source": [
    "# SECTION 1: DATA LOADING AND VALIDATION\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 1: DATA LOADING AND VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize data processor (config is relative to notebook)\n",
    "processor = InsuranceDataProcessor(config_path=\"../config/config.yaml\")\n",
    "\n",
    "# Load data\n",
    "df = processor.load_data()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(f\"\\nData Types Summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Display last few rows\n",
    "print(\"\\nLast 5 rows:\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 2: DATA QUALITY ASSESSMENT\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 2: DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Validate data structure\n",
    "validation_results = processor.validate_data_structure()\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nData Types:\")\n",
    "dtype_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'DataType': df.dtypes.astype(str),\n",
    "    'Non-Null Count': df.count(),\n",
    "    'Null Count': df.isnull().sum(),\n",
    "    'Null Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "display(dtype_summary.sort_values('Null Percentage', ascending=False).head(20))\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"\\nMissing Values Analysis:\")\n",
    "missing_summary = dtype_summary[dtype_summary['Null Percentage'] > 0].sort_values('Null Percentage', ascending=False)\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(f\"Total columns with missing values: {len(missing_summary)}\")\n",
    "    print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"Overall missing percentage: {(df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100):.2f}%\")\n",
    "    \n",
    "    # Plot missing values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_summary.head(20).plot(kind='bar', x='Column', y='Null Percentage', legend=False)\n",
    "    plt.title('Top 20 Columns with Missing Values', fontsize=14)\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Missing Percentage (%)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 3: DATA PREPROCESSING\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 3: DATA PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Preprocess data\n",
    "df_processed = processor.preprocess_data()\n",
    "\n",
    "print(f\"Original shape: {processor.metadata['original_shape']}\")\n",
    "print(f\"Processed shape: {df_processed.shape}\")\n",
    "\n",
    "# Display processed data info\n",
    "print(\"\\nProcessed Data Info:\")\n",
    "print(f\"Memory usage: {df_processed.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df_processed.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicate_count} ({duplicate_count/len(df_processed)*100:.2f}%)\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(\"Removing duplicates...\")\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    print(f\"New shape after removing duplicates: {df_processed.shape}\")\n",
    "\n",
    "# Save processed data\n",
    "processor.save_processed_data()\n",
    "print(\"\\nProcessed data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 4: DESCRIPTIVE STATISTICS\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 4: DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize EDA Analyzer\n",
    "analyzer = EDAAnalyzer(df_processed)\n",
    "\n",
    "# Compute descriptive statistics\n",
    "desc_stats = analyzer.compute_descriptive_statistics()\n",
    "\n",
    "print(\"Descriptive Statistics for Key Numerical Features:\")\n",
    "key_features = ['TotalPremium', 'TotalClaims', 'LossRatio', 'CustomValueEstimate', \n",
    "                'SumInsured', 'CalculatedPremiumPerTerm']\n",
    "\n",
    "key_stats = desc_stats[desc_stats['feature'].isin(key_features)]\n",
    "display(key_stats[['feature', 'mean', 'std', 'min', '25%', 'median', '75%', 'max', \n",
    "                   'skewness', 'kurtosis', 'missing_pct']])\n",
    "\n",
    "# Create summary table\n",
    "summary_table = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Premium', 'Total Claims', 'Average Loss Ratio',\n",
    "        'Average Premium per Policy', 'Average Claim per Policy',\n",
    "        'Total Policies', 'Date Range'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"${df_processed['TotalPremium'].sum():,.0f}\",\n",
    "        f\"${df_processed['TotalClaims'].sum():,.0f}\",\n",
    "        f\"{df_processed['LossRatio'].mean():.3f}\",\n",
    "        f\"${df_processed['TotalPremium'].mean():,.0f}\",\n",
    "        f\"${df_processed['TotalClaims'].mean():,.0f}\",\n",
    "        f\"{df_processed['PolicyID'].nunique() if 'PolicyID' in df_processed.columns else len(df_processed):,.0f}\",\n",
    "        f\"{df_processed['TransactionMonth'].min().date()} to {df_processed['TransactionMonth'].max().date()}\"\n",
    "        if 'TransactionMonth' in df_processed.columns else 'N/A'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nKey Business Metrics Summary:\")\n",
    "display(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1aecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 5: UNIVARIATE ANALYSIS\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 5: UNIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = EDAVisualizer(df_processed, save_dir=\"../reports/figures\")\n",
    "\n",
    "# Analyze distributions\n",
    "distribution_results = analyzer.analyze_distributions()\n",
    "\n",
    "# Plot data quality summary\n",
    "missing_df = analyzer.analyze_missing_values()\n",
    "visualizer.plot_data_quality_summary(missing_df)\n",
    "\n",
    "# Plot numeric distributions\n",
    "numeric_cols = analyzer.numeric_cols[:15]  # First 15 numeric columns\n",
    "print(f\"\\nPlotting distributions for {len(numeric_cols)} numeric columns...\")\n",
    "visualizer.plot_numeric_distributions(numeric_cols)\n",
    "\n",
    "# Plot categorical distributions\n",
    "categorical_cols = analyzer.categorical_cols[:10]  # First 10 categorical columns\n",
    "print(f\"\\nPlotting distributions for {len(categorical_cols)} categorical columns...\")\n",
    "visualizer.plot_categorical_distributions(categorical_cols)\n",
    "\n",
    "# Display distribution insights\n",
    "print(\"\\nDistribution Insights:\")\n",
    "for col, stats in distribution_results.get('numeric_distributions', {}).items():\n",
    "    if col in ['TotalPremium', 'TotalClaims', 'LossRatio']:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean: {stats['mean']:.2f}, Median: {stats['median']:.2f}\")\n",
    "        print(f\"  Coefficient of Variation: {stats['coefficient_of_variation']:.2f}\"\n",
    "              if stats['coefficient_of_variation'] else \"  Coefficient of Variation: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7029922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 6: BIVARIATE/MULTIVARIATE ANALYSIS\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 6: BIVARIATE/MULTIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"Analyzing correlations...\")\n",
    "correlation_matrix = analyzer.analyze_correlations(method='spearman')\n",
    "\n",
    "# Plot correlation matrix\n",
    "visualizer.plot_correlation_matrix(correlation_matrix)\n",
    "\n",
    "# Display highly correlated pairs\n",
    "if 'correlations' in analyzer.results and analyzer.results['correlations'].get('high_correlation_pairs') is not None:\n",
    "    high_corr = analyzer.results['correlations']['high_correlation_pairs']\n",
    "    if len(high_corr) > 0:\n",
    "        print(f\"\\nFound {len(high_corr)} highly correlated pairs (|correlation| > 0.7):\")\n",
    "        display(high_corr.head(10))\n",
    "    else:\n",
    "        print(\"\\nNo highly correlated pairs found (|correlation| > 0.7)\")\n",
    "\n",
    "# Scatter plot matrix for key variables\n",
    "key_vars = ['TotalPremium', 'TotalClaims', 'LossRatio', 'CustomValueEstimate']\n",
    "key_df = df_processed[key_vars].dropna()\n",
    "\n",
    "if len(key_df) > 0:\n",
    "    print(\"\\nCreating scatter plot matrix for key variables...\")\n",
    "    scatter_matrix = pd.plotting.scatter_matrix(key_df, figsize=(12, 12), diagonal='hist', \n",
    "                                               alpha=0.5, grid=True)\n",
    "    plt.suptitle('Scatter Plot Matrix of Key Variables', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../reports/figures/scatter_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Pairwise relationships analysis\n",
    "print(\"\\nAnalyzing pairwise relationships...\")\n",
    "pair_relationships = []\n",
    "\n",
    "for i, var1 in enumerate(key_vars):\n",
    "    for var2 in key_vars[i+1:]:\n",
    "        if var1 in df_processed.columns and var2 in df_processed.columns:\n",
    "            corr = df_processed[[var1, var2]].corr().iloc[0, 1]\n",
    "            pair_relationships.append({\n",
    "                'Variable 1': var1,\n",
    "                'Variable 2': var2,\n",
    "                'Correlation': corr,\n",
    "                'Relationship': 'Strong Positive' if corr > 0.7 else\n",
    "                               'Moderate Positive' if corr > 0.3 else\n",
    "                               'Weak Positive' if corr > 0 else\n",
    "                               'Weak Negative' if corr > -0.3 else\n",
    "                               'Moderate Negative' if corr > -0.7 else\n",
    "                               'Strong Negative'\n",
    "            })\n",
    "\n",
    "pair_df = pd.DataFrame(pair_relationships)\n",
    "print(\"\\nPairwise Relationships Summary:\")\n",
    "display(pair_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 7: LOSS RATIO ANALYSIS\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 7: LOSS RATIO ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate overall loss ratio\n",
    "overall_loss_ratio = df_processed['LossRatio'].mean()\n",
    "print(f\"\\nOverall Portfolio Loss Ratio: {overall_loss_ratio:.3f}\")\n",
    "\n",
    "# Analyze loss ratio by different dimensions\n",
    "dimensions = ['Province', 'VehicleType', 'Gender', 'CoverType', 'Make']\n",
    "dimension_results = analyzer.analyze_by_dimensions(dimensions)\n",
    "\n",
    "# Display loss ratio by province\n",
    "if 'Province' in dimension_results:\n",
    "    print(\"\\nLoss Ratio by Province (Top 10):\")\n",
    "    province_loss = dimension_results['Province'].sort_values('LossRatio_mean', ascending=False).head(10)\n",
    "    display(province_loss[['Province', 'LossRatio_mean', 'TotalPremium_sum', 'TotalClaims_sum', \n",
    "                          'Premium_Share', 'Claims_Share']])\n",
    "\n",
    "# Display loss ratio by vehicle type\n",
    "if 'VehicleType' in dimension_results:\n",
    "    print(\"\\nLoss Ratio by Vehicle Type:\")\n",
    "    vehicle_loss = dimension_results['VehicleType'].sort_values('LossRatio_mean', ascending=False)\n",
    "    display(vehicle_loss[['VehicleType', 'LossRatio_mean', 'TotalPremium_sum', 'TotalClaims_sum']])\n",
    "\n",
    "# Display loss ratio by gender\n",
    "if 'Gender' in dimension_results:\n",
    "    print(\"\\nLoss Ratio by Gender:\")\n",
    "    gender_loss = dimension_results['Gender'].sort_values('LossRatio_mean', ascending=False)\n",
    "    display(gender_loss[['Gender', 'LossRatio_mean', 'TotalPremium_sum', 'TotalClaims_sum']])\n",
    "\n",
    "# Plot loss ratio analysis\n",
    "visualizer.plot_loss_ratio_analysis()\n",
    "\n",
    "# Create detailed loss ratio report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOSS RATIO INSIGHTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "loss_ratio_insights = []\n",
    "\n",
    "# Provincial insights\n",
    "if 'Province' in dimension_results:\n",
    "    highest_province = dimension_results['Province'].loc[dimension_results['Province']['LossRatio_mean'].idxmax()]\n",
    "    lowest_province = dimension_results['Province'].loc[dimension_results['Province']['LossRatio_mean'].idxmin()]\n",
    "    \n",
    "    loss_ratio_insights.append({\n",
    "        'Insight': f\"Highest Loss Ratio Province: {highest_province['Province']}\",\n",
    "        'Value': f\"{highest_province['LossRatio_mean']:.3f}\",\n",
    "        'Interpretation': f\"Claims are {highest_province['LossRatio_mean']/overall_loss_ratio:.1f}x portfolio average\"\n",
    "    })\n",
    "    \n",
    "    loss_ratio_insights.append({\n",
    "        'Insight': f\"Lowest Loss Ratio Province: {lowest_province['Province']}\",\n",
    "        'Value': f\"{lowest_province['LossRatio_mean']:.3f}\",\n",
    "        'Interpretation': f\"Claims are {lowest_province['LossRatio_mean']/overall_loss_ratio:.1f}x portfolio average\"\n",
    "    })\n",
    "\n",
    "# Vehicle type insights\n",
    "if 'VehicleType' in dimension_results:\n",
    "    highest_vehicle = dimension_results['VehicleType'].loc[dimension_results['VehicleType']['LossRatio_mean'].idxmax()]\n",
    "    \n",
    "    loss_ratio_insights.append({\n",
    "        'Insight': f\"Highest Risk Vehicle Type: {highest_vehicle['VehicleType']}\",\n",
    "        'Value': f\"{highest_vehicle['LossRatio_mean']:.3f}\",\n",
    "        'Interpretation': f\"Consider adjusting premiums or coverage for this vehicle type\"\n",
    "    })\n",
    "\n",
    "# Gender insights\n",
    "if 'Gender' in dimension_results:\n",
    "    gender_comparison = dimension_results['Gender']\n",
    "    if len(gender_comparison) >= 2:\n",
    "        gender_diff = gender_comparison['LossRatio_mean'].max() - gender_comparison['LossRatio_mean'].min()\n",
    "        \n",
    "        loss_ratio_insights.append({\n",
    "            'Insight': \"Gender-based Loss Ratio Difference\",\n",
    "            'Value': f\"{gender_diff:.3f}\",\n",
    "            'Interpretation': f\"Significant difference in loss ratios between genders\"\n",
    "        })\n",
    "\n",
    "insights_df = pd.DataFrame(loss_ratio_insights)\n",
    "display(insights_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0336d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 8: OUTLIER DETECTION\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 8: OUTLIER DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Detect outliers\n",
    "outlier_results = analyzer.detect_outliers(method='iqr', threshold=1.5)\n",
    "\n",
    "# Display outlier summary\n",
    "if 'summary' in outlier_results:\n",
    "    summary = outlier_results['summary']\n",
    "    print(f\"\\nOutlier Detection Summary:\")\n",
    "    print(f\"Total records with outliers: {summary['total_outlier_records']:,} \"\n",
    "          f\"({summary['percentage_outlier_records']:.2f}%)\")\n",
    "    print(f\"Features with outliers: {summary['features_with_outliers']}\")\n",
    "\n",
    "# Display top features with most outliers\n",
    "if 'outliers_by_feature' in outlier_results:\n",
    "    outlier_features = []\n",
    "    for feature, stats in outlier_results['outliers_by_feature'].items():\n",
    "        if stats['count'] > 0:\n",
    "            outlier_features.append({\n",
    "                'Feature': feature,\n",
    "                'Outliers': stats['count'],\n",
    "                'Percentage': stats['percentage'],\n",
    "                'Lower Bound': stats.get('lower_bound', 'N/A'),\n",
    "                'Upper Bound': stats.get('upper_bound', 'N/A')\n",
    "            })\n",
    "    \n",
    "    if outlier_features:\n",
    "        outlier_df = pd.DataFrame(outlier_features).sort_values('Outliers', ascending=False)\n",
    "        print(f\"\\nTop 10 Features with Most Outliers:\")\n",
    "        display(outlier_df.head(10))\n",
    "        \n",
    "        # Plot outlier analysis\n",
    "        visualizer.plot_outlier_analysis(outlier_results)\n",
    "        \n",
    "        # Analyze impact of outliers on key metrics\n",
    "        print(\"\\nAnalyzing impact of outliers on key metrics...\")\n",
    "        \n",
    "        for feature in ['TotalClaims', 'TotalPremium', 'CustomValueEstimate']:\n",
    "            if feature in outlier_results['outliers_by_feature']:\n",
    "                stats = outlier_results['outliers_by_feature'][feature]\n",
    "                if stats['count'] > 0:\n",
    "                    outlier_indices = stats['outlier_indices']\n",
    "                    outlier_sum = df_processed.loc[outlier_indices, feature].sum()\n",
    "                    total_sum = df_processed[feature].sum()\n",
    "                    \n",
    "                    print(f\"\\n{feature}:\")\n",
    "                    print(f\"  Outliers contribute: ${outlier_sum:,.0f} \"\n",
    "                          f\"({outlier_sum/total_sum*100:.1f}% of total)\")\n",
    "                    print(f\"  Number of outlier records: {stats['count']:,} \"\n",
    "                          f\"({stats['percentage']:.1f}% of records)\")\n",
    "    else:\n",
    "        print(\"\\nNo outliers detected in any features!\")\n",
    "else:\n",
    "    print(\"\\nOutlier detection results not available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6493a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 9: TEMPORAL TREND ANALYSIS\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 9: TEMPORAL TREND ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze temporal trends\n",
    "temporal_results = analyzer.analyze_temporal_trends()\n",
    "\n",
    "if 'monthly_data' in temporal_results:\n",
    "    monthly_data = temporal_results['monthly_data']\n",
    "    \n",
    "    print(f\"\\nTemporal Analysis Period: {monthly_data.index[0].date()} to {monthly_data.index[-1].date()}\")\n",
    "    print(f\"Number of months analyzed: {len(monthly_data)}\")\n",
    "    \n",
    "    # Display monthly summary\n",
    "    print(\"\\nMonthly Summary Statistics:\")\n",
    "    monthly_summary = monthly_data.describe()\n",
    "    display(monthly_summary)\n",
    "    \n",
    "    # Calculate trends\n",
    "    print(\"\\nTrend Analysis:\")\n",
    "    for key in ['TotalPremium_trend', 'TotalClaims_trend', 'LossRatio_trend']:\n",
    "        if key in temporal_results:\n",
    "            trend = temporal_results[key]\n",
    "            direction = 'üìà Increasing' if trend['slope'] > 0 else 'üìâ Decreasing'\n",
    "            significance = 'Significant' if trend['p_value'] < 0.05 else 'Not Significant'\n",
    "            \n",
    "            print(f\"\\n{key.replace('_trend', '').replace('Total', '')}:\")\n",
    "            print(f\"  Direction: {direction} (slope: {trend['slope']:.4f})\")\n",
    "            print(f\"  R-squared: {trend['r_squared']:.3f}\")\n",
    "            print(f\"  P-value: {trend['p_value']:.4f} ({significance})\")\n",
    "            print(f\"  Percent Change: {trend['percent_change']:.1f}% over period\")\n",
    "    \n",
    "    # Plot temporal trends\n",
    "    visualizer.plot_temporal_trends(monthly_data)\n",
    "    \n",
    "    # Monthly volatility analysis\n",
    "    print(\"\\nMonthly Volatility Analysis:\")\n",
    "    volatility_stats = {\n",
    "        'Metric': ['TotalPremium', 'TotalClaims', 'LossRatio'],\n",
    "        'Std_Dev': [monthly_data['TotalPremium'].std(), \n",
    "                   monthly_data['TotalClaims'].std(), \n",
    "                   monthly_data['LossRatio'].std()],\n",
    "        'CV': [monthly_data['TotalPremium'].std() / monthly_data['TotalPremium'].mean(),\n",
    "              monthly_data['TotalClaims'].std() / monthly_data['TotalClaims'].mean(),\n",
    "              monthly_data['LossRatio'].std() / monthly_data['LossRatio'].mean()]\n",
    "    }\n",
    "    \n",
    "    volatility_df = pd.DataFrame(volatility_stats)\n",
    "    display(volatility_df)\n",
    "    \n",
    "    # Identify peak months\n",
    "    print(\"\\nPeak Months Analysis:\")\n",
    "    peak_months = {\n",
    "        'Highest Premium Month': monthly_data['TotalPremium'].idxmax().strftime('%B %Y'),\n",
    "        'Highest Claims Month': monthly_data['TotalClaims'].idxmax().strftime('%B %Y'),\n",
    "        'Highest Loss Ratio Month': monthly_data['LossRatio'].idxmax().strftime('%B %Y'),\n",
    "        'Lowest Loss Ratio Month': monthly_data['LossRatio'].idxmin().strftime('%B %Y')\n",
    "    }\n",
    "    \n",
    "    for metric, month in peak_months.items():\n",
    "        print(f\"  {metric}: {month}\")\n",
    "else:\n",
    "    print(\"Temporal analysis not available. Check if 'TransactionMonth' column exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 10: VEHICLE MAKE/MODEL ANALYSIS\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 10: VEHICLE MAKE/MODEL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if all(col in df_processed.columns for col in ['Make', 'Model', 'TotalClaims']):\n",
    "    # Analyze vehicle makes and models\n",
    "    vehicle_analysis = df_processed.groupby(['Make', 'Model']).agg({\n",
    "        'TotalClaims': ['sum', 'mean', 'count'],\n",
    "        'TotalPremium': ['sum', 'mean'],\n",
    "        'LossRatio': ['mean', 'std'],\n",
    "        'CustomValueEstimate': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    vehicle_analysis.columns = ['_'.join(col).strip() for col in vehicle_analysis.columns.values]\n",
    "    vehicle_analysis = vehicle_analysis.reset_index()\n",
    "    \n",
    "    # Filter for sufficient data\n",
    "    vehicle_analysis = vehicle_analysis[vehicle_analysis['TotalClaims_count'] > 5]\n",
    "    \n",
    "    print(f\"\\nAnalyzed {len(vehicle_analysis)} unique make/model combinations \"\n",
    "          f\"(with >5 policies each)\")\n",
    "    \n",
    "    # Top 10 by total claims\n",
    "    print(\"\\nTop 10 Vehicle Makes/Models by Total Claims:\")\n",
    "    top_claims = vehicle_analysis.sort_values('TotalClaims_sum', ascending=False).head(10)\n",
    "    display(top_claims[['Make', 'Model', 'TotalClaims_sum', 'TotalClaims_mean', \n",
    "                       'LossRatio_mean', 'TotalClaims_count']])\n",
    "    \n",
    "    # Top 10 by loss ratio\n",
    "    print(\"\\nTop 10 Vehicle Makes/Models by Loss Ratio (Highest Risk):\")\n",
    "    top_risk = vehicle_analysis.sort_values('LossRatio_mean', ascending=False).head(10)\n",
    "    display(top_risk[['Make', 'Model', 'LossRatio_mean', 'TotalClaims_sum', \n",
    "                     'TotalPremium_sum', 'TotalClaims_count']])\n",
    "    \n",
    "    # Bottom 10 by loss ratio\n",
    "    print(\"\\nTop 10 Vehicle Makes/Models by Loss Ratio (Lowest Risk):\")\n",
    "    bottom_risk = vehicle_analysis.sort_values('LossRatio_mean', ascending=True).head(10)\n",
    "    display(bottom_risk[['Make', 'Model', 'LossRatio_mean', 'TotalClaims_sum', \n",
    "                        'TotalPremium_sum', 'TotalClaims_count']])\n",
    "    \n",
    "    # Make-level analysis\n",
    "    print(\"\\nVehicle Make Analysis (Aggregated):\")\n",
    "    make_analysis = df_processed.groupby('Make').agg({\n",
    "        'TotalClaims': 'sum',\n",
    "        'TotalPremium': 'sum',\n",
    "        'LossRatio': 'mean',\n",
    "        'PolicyID': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    make_analysis = make_analysis[make_analysis['PolicyID'] > 10]  # Filter for sufficient data\n",
    "    make_analysis['LossRatio'] = make_analysis['TotalClaims'] / make_analysis['TotalPremium']\n",
    "    make_analysis['Premium_Share'] = (make_analysis['TotalPremium'] / make_analysis['TotalPremium'].sum()) * 100\n",
    "    make_analysis['Claims_Share'] = (make_analysis['TotalClaims'] / make_analysis['TotalClaims'].sum()) * 100\n",
    "    \n",
    "    print(f\"\\nAnalyzed {len(make_analysis)} makes (with >10 policies each)\")\n",
    "    \n",
    "    # Display top risky makes\n",
    "    print(\"\\nTop 10 Risky Makes by Loss Ratio:\")\n",
    "    display(make_analysis.sort_values('LossRatio', ascending=False).head(10)[\n",
    "        ['Make', 'LossRatio', 'TotalClaims', 'TotalPremium', 'PolicyID', 'Claims_Share', 'Premium_Share']\n",
    "    ])\n",
    "    \n",
    "    # Display safest makes\n",
    "    print(\"\\nTop 10 Safest Makes by Loss Ratio:\")\n",
    "    display(make_analysis.sort_values('LossRatio', ascending=True).head(10)[\n",
    "        ['Make', 'LossRatio', 'TotalClaims', 'TotalPremium', 'PolicyID', 'Claims_Share', 'Premium_Share']\n",
    "    ])\n",
    "    \n",
    "    # Create visualizations\n",
    "    # 1. Bar chart of top 10 makes by claims\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    top_makes = make_analysis.sort_values('TotalClaims', ascending=False).head(10)\n",
    "    plt.barh(top_makes['Make'], top_makes['TotalClaims'], color='steelblue')\n",
    "    plt.xlabel('Total Claims ($)')\n",
    "    plt.title('Top 10 Vehicle Makes by Total Claims', fontsize=14)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../reports/figures/top_makes_claims.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Scatter plot: Premium vs Claims by Make\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(make_analysis['TotalPremium'], make_analysis['TotalClaims'], \n",
    "               s=make_analysis['PolicyID']/10, alpha=0.6, c=make_analysis['LossRatio'], \n",
    "               cmap='RdYlGn_r')\n",
    "    plt.colorbar(label='Loss Ratio')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Total Premium (Log Scale)')\n",
    "    plt.ylabel('Total Claims (Log Scale)')\n",
    "    plt.title('Premium vs Claims by Vehicle Make', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate top makes\n",
    "    for idx, row in make_analysis.nlargest(5, 'TotalClaims').iterrows():\n",
    "        plt.annotate(row['Make'], (row['TotalPremium'], row['TotalClaims']),\n",
    "                    fontsize=9, alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../reports/figures/premium_vs_claims_makes.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Vehicle make/model analysis not available. Required columns missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 11: CREATIVE VISUALIZATIONS\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 11: CREATIVE VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nCreating Creative Visualization 1: Risk Heatmap by Province and Vehicle Type\")\n",
    "visualizer.create_creative_visualization_1()\n",
    "\n",
    "print(\"\\nCreating Creative Visualization 2: Interactive Risk Profile Dashboard\")\n",
    "visualizer.create_creative_visualization_2()\n",
    "\n",
    "print(\"\\nCreating Creative Visualization 3: Temporal Risk Evolution Dashboard\")\n",
    "visualizer.create_creative_visualization_3()\n",
    "\n",
    "print(\"\\n‚úÖ All creative visualizations created and saved to reports/figures/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 12: COMPREHENSIVE SUMMARY REPORT\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 12: COMPREHENSIVE SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate comprehensive summary\n",
    "summary_report = analyzer.generate_summary_report()\n",
    "\n",
    "print(\"\\nüìä COMPREHENSIVE EDA SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Dataset Overview\n",
    "print(\"\\n1. DATASET OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ Shape: {summary_report['dataset_overview']['shape']}\")\n",
    "print(f\"   ‚Ä¢ Memory Usage: {summary_report['dataset_overview']['memory_usage_mb']:.2f} MB\")\n",
    "if summary_report['dataset_overview']['date_range']['start']:\n",
    "    print(f\"   ‚Ä¢ Date Range: {summary_report['dataset_overview']['date_range']['start'].date()} \"\n",
    "          f\"to {summary_report['dataset_overview']['date_range']['end'].date()}\")\n",
    "    print(f\"   ‚Ä¢ Duration: {summary_report['dataset_overview']['date_range']['duration_days']} days\")\n",
    "\n",
    "# Data Quality\n",
    "print(\"\\n2. DATA QUALITY:\")\n",
    "print(f\"   ‚Ä¢ Total Missing Values: {summary_report['data_quality']['total_missing_values']:,}\")\n",
    "print(f\"   ‚Ä¢ Missing Percentage: {summary_report['data_quality']['missing_value_percentage']:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Duplicate Rows: {summary_report['data_quality']['duplicate_rows']:,} \"\n",
    "      f\"({summary_report['data_quality']['duplicate_percentage']:.2f}%)\")\n",
    "\n",
    "# Key Metrics\n",
    "print(\"\\n3. KEY BUSINESS METRICS:\")\n",
    "for metric, value in summary_report['key_metrics'].items():\n",
    "    if value is not None:\n",
    "        metric_name = metric.replace('_', ' ').title()\n",
    "        if 'ratio' in metric or 'average' in metric:\n",
    "            print(f\"   ‚Ä¢ {metric_name}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ {metric_name}: ${value:,.0f}\")\n",
    "\n",
    "# Risk Insights\n",
    "print(\"\\n4. RISK INSIGHTS:\")\n",
    "if 'risk_insights' in summary_report:\n",
    "    insights = summary_report['risk_insights']\n",
    "    \n",
    "    if 'top_risky_provinces' in insights:\n",
    "        print(\"\\n   Top 5 Risky Provinces:\")\n",
    "        for province in insights['top_risky_provinces'][:5]:\n",
    "            print(f\"     ‚Ä¢ {province['Province']}: Loss Ratio = {province['mean']:.3f} \"\n",
    "                  f\"({province['count']} policies)\")\n",
    "    \n",
    "    if 'top_risky_vehicles' in insights:\n",
    "        print(\"\\n   Top 5 Risky Vehicle Types:\")\n",
    "        for vehicle in insights['top_risky_vehicles'][:5]:\n",
    "            print(f\"     ‚Ä¢ {vehicle['VehicleType']}: Loss Ratio = {vehicle['mean']:.3f} \"\n",
    "                  f\"({vehicle['count']} policies)\")\n",
    "    \n",
    "    if 'highest_claim_makes_models' in insights:\n",
    "        print(\"\\n   Top 5 High-Claim Vehicle Makes/Models:\")\n",
    "        for vehicle in insights['highest_claim_makes_models'][:5]:\n",
    "            print(f\"     ‚Ä¢ {vehicle['Make']} {vehicle['Model']}: \"\n",
    "                  f\"${vehicle['sum']:,.0f} total claims\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n5. KEY RECOMMENDATIONS:\")\n",
    "print(\"   ‚Ä¢ Review pricing strategy for high-loss-ratio provinces\")\n",
    "print(\"   ‚Ä¢ Investigate risk factors for top risky vehicle types\")\n",
    "print(\"   ‚Ä¢ Monitor temporal trends for seasonal patterns\")\n",
    "print(\"   ‚Ä¢ Consider data quality improvements for columns with high missing values\")\n",
    "print(\"   ‚Ä¢ Further investigate outlier cases in claims and premiums\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ EDA COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save summary report to file\n",
    "import json\n",
    "with open(\"../reports/docs/eda_summary.json\", \"w\") as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nüìÅ Summary report saved to: ../reports/docs/eda_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
