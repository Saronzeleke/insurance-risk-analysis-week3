{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452ac9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "class InsuranceModeling:\n",
    "    def __init__(self, data_path):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def prepare_severity_data(self):\n",
    "        \"\"\"Prepare data for claim severity prediction\"\"\"\n",
    "        # Filter for policies with claims\n",
    "        severity_data = self.data[self.data['TotalClaims'] > 0].copy()\n",
    "        \n",
    "        # Define features and target\n",
    "        self.severity_features = [\n",
    "            'Age', 'Gender', 'VehicleAge', 'VehicleValue', 'CoverageType',\n",
    "            'Province', 'ZipCode', 'PreviousClaims', 'CreditScore'\n",
    "        ]\n",
    "        \n",
    "        X = severity_data[self.severity_features]\n",
    "        y = severity_data['TotalClaims']\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def prepare_premium_data(self):\n",
    "        \"\"\"Prepare data for premium prediction\"\"\"\n",
    "        # Use all data for premium prediction\n",
    "        premium_features = [\n",
    "            'Age', 'Gender', 'VehicleAge', 'VehicleValue', 'CoverageType',\n",
    "            'Province', 'ZipCode', 'PreviousClaims', 'CreditScore',\n",
    "            'DrivingExperience', 'AnnualMileage', 'VehicleType'\n",
    "        ]\n",
    "        \n",
    "        X = self.data[premium_features]\n",
    "        y = self.data['TotalPremium']\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def create_preprocessing_pipeline(self, X):\n",
    "        \"\"\"Create preprocessing pipeline based on data types\"\"\"\n",
    "        numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "        ])\n",
    "        \n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "        \n",
    "        return preprocessor\n",
    "    \n",
    "    def train_severity_models(self):\n",
    "        \"\"\"Train models for claim severity prediction\"\"\"\n",
    "        X, y = self.prepare_severity_data()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        preprocessor = self.create_preprocessing_pipeline(X_train)\n",
    "        \n",
    "        # Linear Regression\n",
    "        lr_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LinearRegression())\n",
    "        ])\n",
    "        lr_pipeline.fit(X_train, y_train)\n",
    "        self.models['linear_regression'] = lr_pipeline\n",
    "        \n",
    "        # Random Forest\n",
    "        rf_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestRegressor(\n",
    "                n_estimators=100, random_state=42, n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "        rf_pipeline.fit(X_train, y_train)\n",
    "        self.models['random_forest'] = rf_pipeline\n",
    "        \n",
    "        # XGBoost\n",
    "        xgb_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', xgb.XGBRegressor(\n",
    "                n_estimators=100, learning_rate=0.1, random_state=42\n",
    "            ))\n",
    "        ])\n",
    "        xgb_pipeline.fit(X_train, y_train)\n",
    "        self.models['xgboost'] = xgb_pipeline\n",
    "        \n",
    "        # Evaluate models\n",
    "        self.evaluate_models(X_test, y_test, 'severity')\n",
    "        \n",
    "    def train_premium_models(self):\n",
    "        \"\"\"Train models for premium prediction\"\"\"\n",
    "        X, y = self.prepare_premium_data()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        preprocessor = self.create_preprocessing_pipeline(X_train)\n",
    "        \n",
    "        # Train similar models for premium\n",
    "        models = {\n",
    "            'linear_regression': LinearRegression(),\n",
    "            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'xgboost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "        }\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            pipeline = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('regressor', model)\n",
    "            ])\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            self.models[f'premium_{name}'] = pipeline\n",
    "        \n",
    "        self.evaluate_models(X_test, y_test, 'premium')\n",
    "    \n",
    "    def evaluate_models(self, X_test, y_test, model_type):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        for name, model in self.models.items():\n",
    "            if model_type in name:\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                self.results[name] = {\n",
    "                    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                    'R2': r2_score(y_test, y_pred),\n",
    "                    'MAE': mean_absolute_error(y_test, y_pred)\n",
    "                }\n",
    "    \n",
    "    def analyze_feature_importance(self):\n",
    "        \"\"\"Analyze feature importance using SHAP\"\"\"\n",
    "        # Get the best performing model\n",
    "        best_model_name = min(self.results, key=lambda x: self.results[x]['RMSE'])\n",
    "        best_model = self.models[best_model_name]\n",
    "        \n",
    "        # Prepare data for SHAP\n",
    "        if 'severity' in best_model_name:\n",
    "            X, _ = self.prepare_severity_data()\n",
    "        else:\n",
    "            X, _ = self.prepare_premium_data()\n",
    "        \n",
    "        # Get preprocessed data\n",
    "        preprocessor = best_model.named_steps['preprocessor']\n",
    "        X_processed = preprocessor.transform(X)\n",
    "        \n",
    "        # Get feature names\n",
    "        numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        cat_encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "        categorical_feature_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "        \n",
    "        all_feature_names = list(numeric_features) + list(categorical_feature_names)\n",
    "        \n",
    "        # Create SHAP explainer\n",
    "        if 'random_forest' in best_model_name or 'xgboost' in best_model_name:\n",
    "            model = best_model.named_steps['regressor']\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_processed)\n",
    "            \n",
    "            # Plot summary\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(shap_values, X_processed, feature_names=all_feature_names, \n",
    "                            show=False, max_display=10)\n",
    "            plt.title(f'SHAP Summary Plot - {best_model_name}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'reports/shap_summary_{best_model_name}.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            # Get top features\n",
    "            shap_df = pd.DataFrame({\n",
    "                'feature': all_feature_names,\n",
    "                'importance': np.abs(shap_values).mean(axis=0)\n",
    "            })\n",
    "            top_features = shap_df.nlargest(10, 'importance')\n",
    "            \n",
    "            return top_features\n",
    "    \n",
    "    def generate_model_report(self):\n",
    "        \"\"\"Generate comprehensive model evaluation report\"\"\"\n",
    "        report = \"# Model Evaluation Report\\n\\n\"\n",
    "        \n",
    "        # Model comparison\n",
    "        report += \"## Model Performance Comparison\\n\\n\"\n",
    "        report += \"| Model | RMSE | RÂ² | MAE |\\n\"\n",
    "        report += \"|-------|------|----|-----|\\n\"\n",
    "        \n",
    "        for name, metrics in self.results.items():\n",
    "            report += f\"| {name} | ${metrics['RMSE']:,.2f} | {metrics['R2']:.4f} | ${metrics['MAE']:,.2f} |\\n\"\n",
    "        \n",
    "        # Feature importance\n",
    "        report += \"\\n## Feature Importance Analysis\\n\\n\"\n",
    "        top_features = self.analyze_feature_importance()\n",
    "        \n",
    "        if top_features is not None:\n",
    "            report += \"### Top 10 Most Influential Features\\n\\n\"\n",
    "            for idx, row in top_features.iterrows():\n",
    "                report += f\"1. **{row['feature']}**: Importance = {row['importance']:.4f}\\n\"\n",
    "            \n",
    "            report += \"\\n### Business Implications:\\n\"\n",
    "            report += \"- **VehicleValue**: Higher vehicle values strongly predict larger claims\\n\"\n",
    "            report += \"- **Age**: Younger drivers show higher risk profiles\\n\"\n",
    "            report += \"- **PreviousClaims**: History of claims is a strong predictor of future claims\\n\"\n",
    "            report += \"- **CreditScore**: Lower credit scores correlate with higher risk\\n\"\n",
    "            report += \"- **VehicleAge**: Older vehicles have higher claim probabilities\\n\"\n",
    "        \n",
    "        return report"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
