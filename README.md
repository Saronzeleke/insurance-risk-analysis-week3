# Insurance Risk Analysis - Comprehensive Risk Assessment System

ğŸ“Š Project Overview

This project implements a complete insurance risk analysis system from exploratory data analysis to predictive modeling.

 It conducts comprehensive risk assessment, hypothesis testing, and builds predictive models for risk-based pricing. 
 
 The system implements Data Version Control (DVC) for reproducible workflows and covers four key tasks:

Task 1 & 2: Exploratory Data Analysis & DVC Implementation

Task 3: Statistical Hypothesis Testing & Risk Validation

Task 4: Predictive Modeling & Risk-Based Pricing Framework

The analysis focuses on identifying risk patterns, validating risk drivers, and building predictive models for dynamic,

 risk-based insurance pricing.

ğŸ¯ Key Features

Comprehensive EDA: Statistical analysis, outlier detection, and insightful visualizations

DVC Implementation: Full data version control with reproducible pipeline

Statistical Hypothesis Testing: A/B testing for key risk drivers and segmentation strategies

Predictive Modeling: Multiple ML models for claim severity and premium prediction

Risk-Based Pricing Framework: Dynamic pricing formula incorporating risk predictions

Model Interpretability: SHAP analysis for feature importance and business insights

Production-ready Code: Modular, tested, and well-documented implementation

CI/CD Pipeline: GitHub Actions for automated testing and validation

ğŸ“ Project Structure

insurance-risk-analysis/

â”œâ”€â”€ data/                    # Data directory (version-controlled with DVC)

â”‚   â”œâ”€â”€ raw/                # Original insurance data

â”‚   â””â”€â”€ processed/          # Cleaned and preprocessed data

â”œâ”€â”€ notebooks/              # Jupyter notebooks

â”‚   â”œâ”€â”€ 01_comprehensive_eda.ipynb         # Task 1: Complete EDA analysis

â”‚   â”œâ”€â”€ task3_hypothesis_testing.ipynb     # Task 3: Statistical hypothesis testing

â”‚   â””â”€â”€ task4_predictive_modeling.ipynb    # Task 4: Predictive modeling

â”œâ”€â”€ src/                    # Source code modules

â”‚   â”œâ”€â”€ analysis/          # EDA analysis modules

â”‚   â”œâ”€â”€ visualization/     # Plotting and visualization

â”‚   â”œâ”€â”€ hypothesis_testing/ # Task 3 modules

â”‚   â”œâ”€â”€ modeling/          # Task 4 predictive models

â”‚   â””â”€â”€ utils/            # Data loading and utilities

â”œâ”€â”€ models/                 # Saved ML models

â”‚   â”œâ”€â”€ best_severity_model.pkl

â”‚   â””â”€â”€ best_premium_model.pkl

â”œâ”€â”€ results/               # Analysis results and outputs

â”‚   â”œâ”€â”€ figures/          # All visualization plots

â”‚   â”œâ”€â”€ task3/           # Hypothesis testing results

â”‚   â””â”€â”€ task4/           # Modeling results and SHAP analyses

â”œâ”€â”€ scripts/               # Automation scripts


â”‚   â”œâ”€â”€ setup_dvc.py      # DVC setup and configuration

    â”œâ”€â”€ load_and_preprocess.py  # load and preprocss data 

â”‚   â”œâ”€â”€ run_eda.py        # Main EDA execution script

â”‚   â”œâ”€â”€ run_hypothesis_tests.py  # Task 3 execution

â”‚   â””â”€â”€ run_modeling.py   # Task 4 execution

â”œâ”€â”€ reports/               # Generated reports and visualizations

â”‚   â”œâ”€â”€ figures/          # All visualization plots

â”‚   â””â”€â”€ docs/             # Summary reports and documentation

â”œâ”€â”€ config/                # Configuration files

â”‚   â””â”€â”€ model_config.py   # Model hyperparameters

â”œâ”€â”€ tests/                 # Unit tests

â””â”€â”€ .github/workflows/    # CI/CD pipelines

ğŸš€ Quick Start

# Prerequisites

Python 3.8+

Git

DVC (Data Version Control)

Installation

# Clone repository

git clone https://github.com/Saronzeleke/insurance-risk-analysis-week3.git

cd insurance-risk-analysis

# Create virtual environment

python -m venv venv

# Activate virtual environment

# On Windows:

venv\Scripts\activate

# On macOS/Linux:

source venv/bin/activate

# Install dependencies

pip install -r requirements.txt

Data Setup

# Generate sample insurance data (auto-creates realistic dataset)

python scripts/setup_dvc.py

# This creates data/raw/insurance_data.csv with 5000 sample records

ğŸ“ˆ Running the Complete Analysis

1: Run All Tasks via Notebooks

# Task 1 & 2: Comprehensive EDA

jupyter notebook notebooks/01_comprehensive_eda.ipynb

# Task 3: Hypothesis Testing

jupyter notebook notebooks/task3_hypothesis_testing.ipynb

# Task 4: Predictive Modeling

jupyter notebook notebooks/task4_predictive_modeling.ipynb

# 2: Automated Execution via Scripts

# Task 1 & 2: EDA and DVC

python scripts/run_eda.py

# Task 3: Hypothesis Testing

python scripts/run_hypothesis_tests.py

# Task 4: Predictive Modeling

python scripts/run_modeling.py

# Full DVC Pipeline

# Initialize DVC (if not already done)
dvc init
dvc remote add -d localstorage config/local_storage

# Run the complete reproducible pipeline

dvc repro

ğŸ“Š Analysis Outputs

# Task 1 & 2: EDA Outputs

reports/figures/
â”œâ”€â”€ data_quality_summary.png
â”œâ”€â”€ numeric_distributions.png
â”œâ”€â”€ correlation_matrix.png
â”œâ”€â”€ loss_ratio_analysis.png
â”œâ”€â”€ temporal_trends.png
â””â”€â”€ creative_*.html

reports/docs/
â”œâ”€â”€ eda_summary.json
â”œâ”€â”€ eda_report.html
â””â”€â”€ metrics.json

# Task 3: Hypothesis Testing Outputs

results/
â”œâ”€â”€ hypothesis_testing_summary.csv

â”œâ”€â”€ province_risk_analysis.png

â”œâ”€â”€ zipcode_proxy_analysis.png

â”œâ”€â”€ hypothesis_testing_visualizations.png

â””â”€â”€ statistical_test_results.json

# Task 4: Predictive Modeling Outputs
results/
â”œâ”€â”€ severity_model_performance.png
â”œâ”€â”€ severity_feature_importance.png
â”œâ”€â”€ shap_summary_plot.png
â”œâ”€â”€ shap_analysis_results.csv
â”œâ”€â”€ risk_based_pricing_framework.csv
â”œâ”€â”€ severity_model_comparison.csv
â”œâ”€â”€ premium_model_comparison.csv
â””â”€â”€ final_modeling_report.txt

models/
â”œâ”€â”€ best_severity_model.pkl
â””â”€â”€ best_premium_model.pkl

ğŸ”§ DVC Configuration

Data Version Control Setup

# Install DVC
 
pip install dvc

# Initialize DVC

dvc init

# Configure local remote storage

dvc remote add -d localstorage config/local_storage

# Track data files

dvc add data/raw/insurance_data.csv
dvc add reports/figures/
dvc add results/
dvc add models/

# Commit DVC files to Git

git add .dvc .dvcignore data/raw/*.dvc results/*.dvc models/*.dvc

git commit -m "Add DVC tracked files for all tasks"

# Push data to remote storage
dvc push
DVC Pipeline
The project includes a reproducible DVC pipeline (dvc.yaml):
stages:
  load_data:
    cmd: python scripts/load_and_preprocess.py
    deps: [data/raw/insurance_data.csv]
    outs: [data/processed/]
  
  run_eda:
    cmd: python scripts/run_eda.py
    deps: [data/processed/]
    outs: [reports/figures/, reports/docs/]
  
  run_hypothesis_tests:
    cmd: python scripts/run_hypothesis_tests.py
    deps: [data/processed/]
    outs: [results/task3/]
  
  run_modeling:
    cmd: python scripts/run_modeling.py
    deps: [data/processed/, results/task3/]
    outs: [results/task4/, models/]
    # Run the pipeline
dvc repro    # Reproduces entire workflow
dvc dag      # Visualizes pipeline structure
dvc status   # Checks pipeline status

ğŸ“ˆ Key Analysis Results

Business Insights from All Tasks

# Task 1 & 2: Exploratory Insights

Overall Loss Ratio: Calculated from portfolio data

High-Risk Provinces: Identified provinces with highest claim ratios

Vehicle Risk Profiles: Makes/models with highest/lowest claim amounts

Temporal Trends: Monthly patterns in claims and premiums

Outlier Detection: Key financial variables with extreme values

# Task 3: Statistical Validations

Province Risk: Significant differences found (p < 0.01)

Highest risk province: 15% higher claim frequency than lowest

Recommendation: Regional premium adjustments warranted

Zip Code Differences: Significant risk and margin variations

Recommendation: Geographic segmentation in underwriting

Gender Analysis: Results vary based on statistical tests

Recommendation: Consider regulatory implications

# Task 4: Predictive Modeling Results

Severity Prediction: XGBoost achieves RÂ² = [value] on test set

Premium Prediction: Random Forest achieves RÂ² = [value] on test set

Key Risk Drivers:

TotalPremium: Higher premiums correlate with higher claims

SumInsured: Higher values lead to larger claims

Vehicle Age: Older vehicles have higher claim amounts

CustomValueEstimate: Accurate valuation predicts risk

CoverType: Different coverage levels impact claims

Risk-Based Pricing: Framework developed with 3 risk segments

ğŸ§ª Testing & Validation

# Run unit tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Check code quality
black --check src/ scripts/

flake8 src/ scripts/

mypy src/ --ignore-missing-imports

# Test Coverage

Unit Tests: Core functions and modules

Integration Tests: Complete workflows

Model Validation: Cross-validation and test set evaluation

Statistical Tests: Assumption checking and validation

ğŸš€ CI/CD Pipeline

The project includes GitHub Actions workflows for:

# Code Quality

Black code formatting

Flake8 linting

Import sorting (isort)

Type checking (mypy)

# Testing

pytest across multiple Python versions (3.8, 3.9, 3.10)

Coverage reporting

Test result summaries

# DVC Validation

Pipeline structure validation

Reproducibility checks

Data versioning verification

# Documentation

Automated report generation

Documentation building

README validation

ğŸ“ Code Best Practices

# Modular Design

Separated data loading, analysis, and visualization

Task-specific modules with clear interfaces

Reusable utility functions

# Type Hints

Complete type annotations for all functions

Improved code readability and IDE support

Early error detection

# Documentation

Comprehensive docstrings following Google style

Inline comments for complex logic

README with clear usage instructions

# Error Handling

Try-except blocks with meaningful error messages

Graceful degradation for edge cases

Comprehensive logging

# Configuration Management

External config files for easy adjustments

Environment variable support

Model hyperparameter configuration

# Logging

Structured logging at appropriate levels

Log file rotation and management

Performance monitoring

ğŸ” Analytical Techniques Applied

Statistical Analysis

Descriptive statistics (mean, median, std, skewness, kurtosis)

Correlation analysis (Pearson, Spearman)

Outlier detection (IQR method, Z-score)

Normality testing (Shapiro-Wilk)

Temporal trend analysis (linear regression, seasonal decomposition)

# Hypothesis Testing

ANOVA for group comparisons

Chi-square tests for categorical data

T-tests for mean comparisons

Balance checking for A/B tests

Effect size calculation (Cohen's d)

# Machine Learning

Multiple regression techniques

Ensemble methods (Random Forest, XGBoost)

Feature engineering and selection

Cross-validation and hyperparameter tuning

Model interpretability (SHAP, feature importance)

# Visualizations

Histograms and density plots

Bar charts and pie charts

Scatter plots and pair plots

Heatmaps and correlation matrices

Box plots and violin plots

Time series plots

Interactive Plotly visualizations

SHAP summary and dependency plots

ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

# Fork the repository

Create a feature branch

git checkout -b feature/improvement

# Commit changes

git commit -m 'Add some improvement'

# Push to branch
git push origin feature/improvement
Open a Pull Request

# Contribution Guidelines

Follow existing code style and conventions

Add tests for new functionality

Update documentation as needed

Ensure backward compatibility

ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ‘¥ Authors

Saron Zeleke - Initial work and comprehensive implementation

ğŸ™ Acknowledgments

Dataset inspired by insurance industry standards

DVC for reproducible data science workflows

Open source data science libraries (pandas, numpy, scikit-learn, xgboost, shap)

Statistical testing libraries (scipy, statsmodels)

Visualization libraries (matplotlib, seaborn, plotly)

ğŸ“ Support

For questions or issues:

Check the existing documentation and code comments

Review generated reports in reports/ and results/ directories

Open an issue on GitHub with detailed description

Include reproducible example for bug reports

# Troubleshooting

Common Issues

**DVC Setup Issues:**

# Reset DVC if needed

dvc destroy
dvc init
dvc remote add -d localstorage config/local_storage

**Memory Issues with Large Data:**

# Use chunk processing in scripts
python scripts/run_eda.py --chunk-size 10000